{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib notebook\n",
    "\n",
    "import sys\n",
    "import numpy as np  # Matrix and vector computation package\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "from matplotlib import cm  # Colormaps\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns  # Fancier plots\n",
    "\n",
    "# Set seaborn plotting style\n",
    "sns.set_style('darkgrid')\n",
    "# Set the seed for reproducability\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "nb_of_samples = 20\n",
    "sequence_len = 10\n",
    "# Create the sequences\n",
    "X = np.zeros((nb_of_samples, sequence_len))\n",
    "for row_idx in range(nb_of_samples):\n",
    "    X[row_idx,:] = np.around(np.random.rand(sequence_len)).astype(int)\n",
    "# Create the targets for each sequence\n",
    "t = np.sum(X, axis=1)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 4., 6., 6., 4., 4., 6., 5., 7., 5., 7., 6., 4., 7., 3., 6., 4.,\n",
       "       5., 6., 6.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward step functions\n",
    "\n",
    "def update_state(xk, sk, wx, wRec):\n",
    "    \"\"\"\n",
    "    Compute state k from the previous state (sk) and current \n",
    "    input (xk), by use of the input weights (wx) and recursive \n",
    "    weights (wRec).\n",
    "    \"\"\"\n",
    "    return xk * wx + sk * wRec\n",
    "\n",
    "\n",
    "def forward_states(X, wx, wRec):\n",
    "    \"\"\"\n",
    "    Unfold the network and compute all state activations \n",
    "    given the input X, input weights (wx), and recursive weights \n",
    "    (wRec). Return the state activations in a matrix, the last \n",
    "    column S[:,-1] contains the final activations.\n",
    "    \"\"\"\n",
    "    # Initialise the matrix that holds all states for all \n",
    "    #  input sequences. The initial state s0 is set to 0.\n",
    "    S = np.zeros((X.shape[0], X.shape[1]+1))\n",
    "    # Use the recurrence relation defined by update_state to update \n",
    "    #  the states trough time.\n",
    "    for k in range(0, X.shape[1]):\n",
    "        # S[k] = S[k-1] * wRec + X[k] * wx\n",
    "        S[:,k+1] = update_state(X[:,k], S[:,k], wx, wRec)\n",
    "    return S\n",
    "\n",
    "\n",
    "def loss(y, t): \n",
    "    \"\"\"MSE between the targets t and the outputs y.\"\"\"\n",
    "    return np.mean((t - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_gradient(y, t):\n",
    "    \"\"\"\n",
    "    Gradient of the MSE loss function with respect to the output y.\n",
    "    \"\"\"\n",
    "    return 2. * (y - t)\n",
    "\n",
    "\n",
    "def backward_gradient(X, S, grad_out, wRec):\n",
    "    \"\"\"\n",
    "    Backpropagate the gradient computed at the output (grad_out) \n",
    "    through the network. Accumulate the parameter gradients for \n",
    "    wX and wRec by for each layer by addition. Return the parameter \n",
    "    gradients as a tuple, and the gradients at the output of each layer.\n",
    "    \"\"\"\n",
    "    # Initialise the array that stores the gradients of the loss with \n",
    "    #  respect to the states.\n",
    "    grad_over_time = np.zeros((X.shape[0], X.shape[1]+1))\n",
    "    grad_over_time[:,-1] = grad_out\n",
    "    # Set the gradient accumulations to 0\n",
    "    wx_grad = 0\n",
    "    wRec_grad = 0\n",
    "    for k in range(X.shape[1], 0, -1):\n",
    "        # Compute the parameter gradients and accumulate the results.\n",
    "        wx_grad += np.sum(\n",
    "            np.mean(grad_over_time[:,k] * X[:,k-1], axis=0))\n",
    "        wRec_grad += np.sum(\n",
    "            np.mean(grad_over_time[:,k] * S[:,k-1]), axis=0)\n",
    "        # Compute the gradient at the output of the previous layer\n",
    "        grad_over_time[:,k-1] = grad_over_time[:,k] * wRec\n",
    "    return (wx_grad, wRec_grad), grad_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gradient errors found\n"
     ]
    }
   ],
   "source": [
    "# Perform gradient checking\n",
    "# Set the weight parameters used during gradient checking\n",
    "params = [1.2, 1.2]  # [wx, wRec]\n",
    "# Set the small change to compute the numerical gradient\n",
    "eps = 1e-7\n",
    "# Compute the backprop gradients\n",
    "S = forward_states(X, params[0], params[1])\n",
    "grad_out = output_gradient(S[:,-1], t)\n",
    "backprop_grads, grad_over_time = backward_gradient(\n",
    "    X, S, grad_out, params[1])\n",
    "# Compute the numerical gradient for each parameter in the layer\n",
    "for p_idx, _ in enumerate(params):\n",
    "    grad_backprop = backprop_grads[p_idx]\n",
    "    # + eps\n",
    "    params[p_idx] += eps\n",
    "    plus_loss = loss(forward_states(X, params[0], params[1])[:,-1], t)\n",
    "    # - eps\n",
    "    params[p_idx] -= 2 * eps\n",
    "    min_loss = loss(forward_states(X, params[0], params[1])[:,-1], t)\n",
    "    # reset param value\n",
    "    params[p_idx] += eps\n",
    "    # calculate numerical gradient\n",
    "    grad_num = (plus_loss - min_loss) / (2*eps)\n",
    "    # Raise error if the numerical grade is not close to \n",
    "    #  the backprop gradient\n",
    "    if not np.isclose(grad_num, grad_backprop):\n",
    "        raise ValueError((\n",
    "            f'Numerical gradient of {grad_num:.6f} is not close to '\n",
    "            f'the backpropagation gradient of {grad_backprop:.6f}!'))\n",
    "print('No gradient errors found')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Rprop optimisation function\n",
    "def update_rprop(X, t, W, W_prev_sign, W_delta, eta_p, eta_n):\n",
    "    \"\"\"\n",
    "    Update Rprop values in one iteration.\n",
    "    Args:\n",
    "        X: input data.\n",
    "        t: targets.\n",
    "        W: Current weight parameters.\n",
    "        W_prev_sign: Previous sign of the W gradient.\n",
    "        W_delta: Rprop update values (Delta).\n",
    "        eta_p, eta_n: Rprop hyperparameters.\n",
    "    Returns:\n",
    "        (W_delta, W_sign): Weight update and sign of last weight\n",
    "                           gradient.\n",
    "    \"\"\"\n",
    "    # Perform forward and backward pass to get the gradients\n",
    "    S = forward_states(X, W[0], W[1])\n",
    "    grad_out = output_gradient(S[:,-1], t)\n",
    "    W_grads, _ = backward_gradient(X, S, grad_out, W[1])\n",
    "    W_sign = np.sign(W_grads)  # Sign of new gradient\n",
    "    # Update the Delta (update value) for each weight \n",
    "    #  parameter seperately\n",
    "    for i, _ in enumerate(W):\n",
    "        if W_sign[i] == W_prev_sign[i]:\n",
    "            W_delta[i] *= eta_p\n",
    "        else:\n",
    "            W_delta[i] *= eta_n\n",
    "    return W_delta, W_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights are: wx = 1.0014,  wRec = 0.9997\n"
     ]
    }
   ],
   "source": [
    "# Perform Rprop optimisation\n",
    "\n",
    "# Set hyperparameters\n",
    "eta_p = 1.2\n",
    "eta_n = 0.5\n",
    "\n",
    "# Set initial parameters\n",
    "W = [-1.5, 2]  # [wx, wRec]\n",
    "W_delta = [0.001, 0.001]  # Update values (Delta) for W\n",
    "W_sign = [0, 0]  # Previous sign of W\n",
    "\n",
    "ls_of_ws = [(W[0], W[1])]  # List of weights to plot\n",
    "# Iterate over 500 iterations\n",
    "for i in range(500):\n",
    "    # Get the update values and sign of the last gradient\n",
    "    W_delta, W_sign = update_rprop(\n",
    "        X, t, W, W_sign, W_delta, eta_p, eta_n)\n",
    "    # Update each weight parameter seperately\n",
    "    for i, _ in enumerate(W):\n",
    "        W[i] -= W_sign[i] * W_delta[i]\n",
    "    ls_of_ws.append((W[0], W[1]))  # Add weights to list to plot\n",
    "\n",
    "print(f'Final weights are: wx = {W[0]:.4f},  wRec = {W[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inpt = np.asmatrix([[1] * 100])\n",
    "test_outpt = forward_states(test_inpt, W[0], W[1])[:,-1]\n",
    "test_outpt\n",
    "# sum_test_inpt = test_inpt.sum()\n",
    "# print((\n",
    "#     f'Target output: {sum_test_inpt:d} vs Model output: '\n",
    "#     f'{test_outpt[0]:.2f}'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
